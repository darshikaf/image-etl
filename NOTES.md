
# Discussion addressing problem statement

## Approach

As mentioned in the `README` the data structure of the final output is inspired by COCO dataset. Currently each unique record will contain the following features:

* classes detected
* list of coordinates where the class is present.
* pixel size of the original image.
* bottom left corner of overall area where classes are detected
* height of the overall area where classes are detected from bottom left corner.
* width of the overall area where classes are detected from bottom left corner.

### Workflow

The workflow generating the fina output was as follows:

* For a batch of input data, group the data based on unique `location id` and `date`.
* Collate all the jobs done on a unique data point to create a unified set of labels.
* Create the above mentioned features for each unique data point, using the unified set of labels.

### Assumption

* Data will be consumed as batches.
* Only training data is processed.
* The requirement is to only prepare a transformed dataset from raw data.

### Limitations

* Performance of the application is not tuned. Therefore I/O operations are not as efficient as they could be. 
* Test coverage is only for `extraction` part.
* Current implementation requires human intervention to run the application. However it could be easily extended so that a machine calls the application periodically, or when a new batch of data is detected.

### Improvements

* I/O operations can be made more efficient through an ExecutorPool. For the purpose of the test this is not implemented.

* During it was intended to embed more information to each record as illustrated below. This approach was considered because the output generated can easily be leveraged for semantic segmentation.

```json
{
    "segmentation": "<list of lists of x,y coordinates for each segment>",
    "image_id": 1,
    "category_id": "class",
    "bbox": "<bounding box for each segment>",
    "area": "<area>"
}
```
This could be generated by applying the following method to :

```python
import numpy as np                                 
from skimage import measure                        
from shapely.geometry import Polygon, MultiPolygon 

def produce_final_output(submask, category_id, image_id)
    contours = measure.find_contours(sub_mask, 0.5, positive_orientation='low')

    segmentations = []
    polygons = []
    for contour in contours:
        for i in range(len(contour)):
            row, col = contour[i]
            contour[i] = (col - 1, row - 1)

        # Make a polygon and simplify it
        poly = Polygon(contour)
        poly = poly.simplify(1.0, preserve_topology=False)
        polygons.append(poly)
        segmentation = np.array(poly.exterior.coords).ravel().tolist()
        segmentations.append(segmentation)

    # Combine the polygons to calculate the bounding box and area
    multi_poly = MultiPolygon(polygons)
    x, y, max_x, max_y = multi_poly.bounds
    width = max_x - x
    height = max_y - y
    bbox = (x, y, width, height)
    area = multi_poly.area

    annotation = {
        'segmentation': segmentations,
        'image_id': image_id,
        'category_id': category_id,
        'bbox': bbox,
        'area': area
    }
```

However due to the sparse nature of labels in sample data, this method could not be applied. If the value of each pixel could be updated using a marching algorithm so that `_connectivity_` for pixels, for a given segment are labeled with same class, this approach could be used for a better output.
